---
title: "project"
author: "Erin Lyons, Yujian Tang, Norma Techarukpong, Ryan Thomas, Jason Yu"
date: "April 3, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(neuralnet)) { install.packages("neuralnet", repos = "http://cran.us.r-project.org")}  
```

```{r}
library(dplyr)
library(MASS)
library(ggplot2)
library(purrr)
library(leaps)
library(glmnet)
library(neuralnet)
library(tidyr)
library(pls)
```

#Data Cleaning
```{r}
#Uploading Dataset
CPS.data <- read.csv("Chicago_Public_Schools_-_Progress_Report_Cards__2011-2012_.csv")

#Cleaning Data
CPS.data$Link <- NULL
CPS.data$Phone.Number <- NULL
CPS.data$State <- NULL
CPS.data$Street.Address <- NULL
CPS.data$Location <- NULL
CPS.data$City <- NULL
CPS.data$RCDTS.Code <- NULL
CPS.data$Latitude <- NULL
CPS.data$Longitude <- NULL
CPS.data$X_COORDINATE <- NULL
CPS.data$Y_COORDINATE <- NULL
```

```{r}
#Removing NDA values from variables and changing the values to NA
for(i in names(CPS.data)) {
  for(j in 1:nrow(CPS.data)) {
    if(isTRUE(CPS.data[j,i] == "NDA")) { CPS.data[j, i] = NA}
  }
}

#View
head(CPS.data)
```
```{r}
#Data that only describes the numeric variables
numeric.data <- CPS.data[, map_lgl(CPS.data, is.numeric)]
head(numeric.data)
```

# Erin's section
```{r }
#preparing data for logistic regression
CPS.data1 <- CPS.data %>%
  filter(!is.na(Adequate.Yearly.Progress.Made.)) %>%
  mutate(y = ifelse(Adequate.Yearly.Progress.Made. == "Yes", 1, 0))
CPS.data1$Adequate.Yearly.Progress.Made. <- NULL
numeric.data1 <- CPS.data1[, map_lgl(CPS.data1, is.numeric)]
numeric.data1$School.ID <- NULL
numeric.data1$ISAT.Value.Add.Math <- NULL
numeric.data1$ISAT.Value.Add.Read <- NULL

numeric.data1 <- na.omit(numeric.data1)

#logistic regression to predict yearly progress

bestglm::bestglm(numeric.data1, family = binomial)
```

```{r}
#best model found above
logit.mod <- glm(y ~  Average.Student.Attendance + ISAT.Exceeding.Reading.. + College.Enrollment..number.of.students., data = numeric.data1, family = "binomial")
summary(logit.mod)
```

```{r}
#test for goodness of fit using g-statistic
gstatistic = 359.23 - 148.22 
df = 3
1.00 - pchisq(gstatistic, df)
```

```{r}
#confusion matrix for predicted values
confusion = function(yhat, y, quietly = F){
if(!quietly)
message("yhat is the vector of predicted outcomes, possibly a factor.\n
Sensitivity = (first level predicted) / (first level actual) \n
Specificity = (second level predicted) / (second level actual)")
if(!is.factor(y) & is.factor(yhat))
y = as.factor(y)
if(!all.equal(levels(yhat), levels(y)))
stop("Factor levels of yhat and y do not match.")
confusion_mat = table(yhat, y, deparse.level = 2)
stats = data.frame(sensitivity = confusion_mat[1, 1]/sum(confusion_mat[, 1]),
specificity = confusion_mat[2, 2]/sum(confusion_mat[, 2]))
return(list(confusion_mat = confusion_mat, stats = stats))
}

confusion(round(logit.mod$fitted.values), (numeric.data1$y), quietly = T)
```

```{r}
#k nearest neighbors model - cross validation using different values of K 
knn_mods <- list()
set.seed(1234)
numeric.data2 <- numeric.data1
numeric.data2$y <- NULL
k.data <- sort(sample(nrow(numeric.data2), nrow(numeric.data2)*.75))
k.train <- numeric.data2[k.data,]
k.test <- numeric.data2[-k.data,]

for (i in 1:10){
  knn_mods[[i]] = knn(k.train, k.test, cl = numeric.data1[k.data,]$y, k = i)
}

knn_results = lapply(knn_mods, FUN = function(x){
return(confusion(x, numeric.data1[-k.data,]$y, quietly = TRUE)$stats)
 })

knn_results = bind_rows(knn_results)
knn_results$K = 1:10
ggplot(knn_results, aes(x = specificity, y = sensitivity, label = K)) + geom_point() + geom_text(hjust = 2)
```
```{r}
knn.mod = knn(k.train, k.test, cl = numeric.data1[k.data,]$y, k = 1)
confusion(knn.mod, numeric.data1[-k.data,]$y )
```

```{r}
# Charles' section
# set seed
set.seed(100)
# First obtain numeric values only
numcols <- unlist(lapply(CPS.data, is.numeric))
CPS.num <- na.omit(CPS.data[,numcols])
# Second check correlations
corrs <- cor(CPS.num)
corrs[upper.tri(corrs)] <- 0
diag(corrs) <- 0
CPS.num.new <- scale(CPS.num[,!apply(corrs, 2, function(x) any (abs(x) > 0.7))])
train_size <- floor(0.7*nrow(CPS.num.new))
train_ind <- sample(seq_len(nrow(CPS.num.new)), size = train_size)
CPS_train <- CPS.num.new[train_ind,]
CPS_test <- CPS.num.new[-train_ind,]

# use a simple neural network to predict Instruction Score
nn1 = neuralnet(Instruction.Score ~ ., data=CPS_train)

# check we get somewhat reasonable predictions
nnpred <- compute(nn1, CPS_test)
nnpred1_results <- ifelse(nnpred$net.result > iscore_med, 1, 0)
aboveavg <- ifelse(CPS_test[,3] > iscore_med, 1, 0)
sum(abs(nnpred1_results - aboveavg))
# we see our naive neural network doesn't perform very well, misclassifying 50/131 test points

```

```{r}
# Norma's section
```


#Ryan's Section
Ideas:

- Other Variables connection to classificaiton
- Are there certain ways to categorize elementary middle and high school?

```{r}
#Safety and Parents
ggplot(CPS.data) +
  geom_point(aes(x = Safety.Score, y = Parent.Environment.Score, col = Adequate.Yearly.Progress.Made.))

#Attendance
ggplot(filter(CPS.data, Average.Teacher.Attendance != 0)) +
  geom_point(aes(x = Average.Student.Attendance, y = Average.Teacher.Attendance, col = Adequate.Yearly.Progress.Made.))

#
```
1st graph: High safety scores indicate higher progress.

2nd graph: Most progress happened when attendance was high for both parties. 

```{r}
head(numeric.data)
```

#Numerical Analysis
```{r}
#Correlation of all Variables
correlation.matrix <- cor(numeric.data, use = "complete.obs")
correlation.dataset <- as.data.frame(correlation.matrix)
most.correlation <- which(abs(correlation.matrix) < 1 & abs(correlation.matrix) > .70)

rows.list <- list()
columns.list <- list()
j <- 1
#For Loop for finding maximum correlations
for(i in most.correlation) {
  #Correlation
  row <- ceiling(i/ncol(numeric.data))
  column <- ifelse(i%%ncol(numeric.data) == 0, ncol(numeric.data), i%%ncol(numeric.data))
  rows.list[[j]] <- rownames(correlation.dataset[row,])
  columns.list[[j]] <- names(correlation.dataset)[column]
  j = j+1
}

row.column.names = cbind(rows.list, columns.list)

#New Dataframe To hold Rows/ Columns
row.column.dataframe <- data.frame(
  row = NULL,
  column = NULL
)
#putting values in dataframe
for(i in 1:length(rows.list)) {
  newRow = data.frame(row = rows.list[[i]], column = columns.list[[i]])
  row.column.dataframe <- rbind(row.column.dataframe, newRow)
}

#Changing variable types to character
row.column.dataframe$row <- as.character(row.column.dataframe$row)
row.column.dataframe$column <- as.character(row.column.dataframe$column)

#Removing Duplicates
for (i in 1:nrow(row.column.dataframe))
{
    row.column.dataframe[i, ] = sort(row.column.dataframe[i, ])
}
row.column.dataframe <- row.column.dataframe[!duplicated(row.column.dataframe),]

#Viewing Dataframe of Variables with the most Correlation
row.column.dataframe
```



```{r}
#For Loop For Creating Scatterplots of data with the Greatest Correlation
for(i in 1:nrow(row.column.dataframe)) {
  #Extract column and row variable
  row <- row.column.dataframe[[i, 1]]
  column <- row.column.dataframe[[i, 2]]
  #Plot
  print(ggplot(CPS.data) +
    geom_point(aes_string(x = row, y = column), color = "blue4") +
    geom_smooth(aes_string(x = row, y = column)) +
    theme_minimal() +
    ggtitle("Correlations"))
}
```

##How well can we predict student attendance?
Goal - determine best model between lasso regression, ridge regression, and least squares regression with variable selection.



```{r}
which.model <- function(vrbl){
full.error <- rep(0, 50)
backwards.error <- rep(0, 50)
forwards.error <- rep(0, 50)
lasso.error <- rep(0, 50)
ridge.error <- rep(0, 50)
pcr.error <- rep(0, 50)

#Data frame with no NA values
no.na <- na.omit(numeric.data)

for(i in 1:50){
test.set <- sample_n(no.na, 100)
train.set <- suppressMessages(anti_join(no.na, test.set))

#Least Squares Model
fullLinMod.SA <- lm(as.formula(paste(vrbl, "~.")), data = train.set)

#Stepwise
backwards <- step(fullLinMod.SA, direction = "backward", trace = FALSE)
forwards <- step(fullLinMod.SA, direction = "backward", trace = FALSE)

#Lasso Regression
modMatrix <- model.matrix(as.formula(paste(vrbl, "~.")), data= train.set)
lassoMod = cv.glmnet(modMatrix, y = train.set[,vrbl], alpha = 1, nfolds = 5)

#Ridge Regression
ridgeMod = cv.glmnet(modMatrix, y = train.set[,vrbl], alpha = 0, nfolds = 5)

#PCR
pcrMod <- pcr(as.formula(paste(vrbl, "~.")), data=train.set, scale=TRUE)

#predictions
full.predictions <- predict(fullLinMod.SA, test.set)
backwards.predictions <- predict(backwards, test.set)
forwards.predictions <- predict(forwards, test.set)

newX <- model.matrix(as.formula(paste("~.-", vrbl)), data=test.set)
lasso.predictions <- predict(lassoMod, s= lassoMod$lambda.min, newx= newX)
ridge.predictions <- predict(ridgeMod, s= ridgeMod$lambda.min, newx= newX)
pcr.predictions <- predict(pcrMod, test.set)

MSE.full = mean((full.predictions - test.set[,vrbl])^2)
MSE.back = mean((backwards.predictions - test.set[,vrbl])^2)
MSE.for = mean((forwards.predictions - test.set[,vrbl])^2)
MSE.lasso = mean((lasso.predictions - test.set[,vrbl])^2)
MSE.ridge = mean((ridge.predictions - test.set[,vrbl])^2)
MSE.pcr = mean((pcr.predictions - test.set[,vrbl])^2)

full.error[[i]] <- MSE.full
backwards.error[[i]] <- MSE.back
forwards.error[[i]] <- MSE.for
lasso.error[[i]] <- MSE.lasso
ridge.error[[i]] <- MSE.ridge
pcr.error[[i]] <- MSE.pcr
}

data = gather(data.frame(Full_Model = full.error,
                  Backwards  = backwards.error,
                  Forwards = forwards.error,
                  Lasso = lasso.error,
                  Ridge = ridge.error,
                  PCR = pcr.error), key = "Method", value = "MSPE")
print(ggplot(data) + 
  geom_boxplot(aes(x = Method, y = MSPE, fill = Method)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))) +
  ggtitle(paste("Model Errors for Predicting ", vrbl))
}
```

##Student Attendance
```{r}
set.seed(123)
which.model("Average.Student.Attendance")
```

##Safety Score Best Model
```{r}
set.seed(123)
which.model("Safety.Score")
```

##Environment Score Best Model
```{r}
set.seed(123)
which.model("Environment.Score")
```

##Instruction Score Best Model
```{r}
set.seed(123)
which.model("Environment.Score")
```

#Rate of Misconduct
```{r}
set.seed(123)
which.model("Rate.of.Misconducts..per.100.students.")
```



##Classification for School Grade-level
```{r}
#Function that removes collinearity for data
remove.collinearity <- function(data, c) {
  #Correlation of all Variables
  correlation.matrix <- cor(data, use = "complete.obs")
  correlation.dataset <- as.data.frame(correlation.matrix)
  most.correlation <- which(abs(correlation.matrix) < 1 & abs(correlation.matrix) > c)

  rows.list <- list()
  columns.list <- list()
  j <- 1
  #For Loop for finding maximum correlations
  for(i in most.correlation) {
    #Correlation
    row <- ceiling(i/ncol(data))
    column <- ifelse(i%%ncol(data) == 0, ncol(data), i%%ncol(data))
    rows.list[[j]] <- rownames(correlation.dataset[row,])
    columns.list[[j]] <- names(correlation.dataset)[column]
    j = j+1
  }

  row.column.names = cbind(rows.list, columns.list)

  #New Dataframe To hold Rows/ Columns
  row.column.dataframe <- data.frame(
    row = NULL,
    column = NULL
  )
  #putting values in dataframe
  for(i in 1:length(rows.list)) {
    newRow = data.frame(row = rows.list[[i]], column = columns.list[[i]])
    row.column.dataframe <- rbind(row.column.dataframe, newRow)
  }

  #Changing variable types to character
  row.column.dataframe$row <- as.character(row.column.dataframe$row)
  row.column.dataframe$column <- as.character(row.column.dataframe$column)

  #Removing Duplicates
  for (i in 1:nrow(row.column.dataframe))
  {
      row.column.dataframe[i, ] = sort(row.column.dataframe[i, ])
  }
  row.column.dataframe <- row.column.dataframe[!duplicated(row.column.dataframe),]

  #Dataframe of Variables with the most Correlation
  table <- row.column.dataframe
  
  #creates new dataframe without correlations above c
  new.data = data
  for(i in 1:nrow(table)) {
    new.data[,table[i,2]] <- NULL
  }
  return(new.data)
}
```

```{r}
newdata = numeric.data %>%
  select_if(Negate(is.integer)) %>%
  select_if(is.numeric)

newdata <- remove.collinearity(newdata, .1)
newdata
newdata <- cbind(numeric.data, CPS.data$Elementary..Middle..or.High.School)
newdata$Elementary..Middle..or.High.School <- newdata$`CPS.data$Elementary..Middle..or.High.School`
newdata$`CPS.data$Elementary..Middle..or.High.School`<- NULL

ldaMod <- lda(Elementary..Middle..or.High.School~., data = newdata)
```
```{r}
names(data.progress)
```

#PCA to hopefully find separation in categories
```{r}
data.progress <- cbind(CPS.data$Adequate.Yearly.Progress.Made.,numeric.data)
data.progress$Adequate.Yearly.Progress.Made. <- data.progress[,"CPS.data$Adequate.Yearly.Progress.Made."]
data.progress[,"CPS.data$Adequate.Yearly.Progress.Made."] <- NULL
data.progress[,"ZIP.Code"] <- NULL
data.progress[,"Community.Area.Number"] <- NULL
data.progress[,"CPS.data"] <- NULL
data.progress[,"School.ID"] <- NULL
data.progress[,"Ward"] <- NULL
data.progress[,"General.Services.Route" ] <- NULL
data.progress[,"Police.District" ] <- NULL
data.progress <- na.omit(data.progress)

PCA.cps <- prcomp(data.progress[1:12], center = TRUE, scale. = TRUE, rank = 3)

plot(PCA.cps)

ggbiplot(PCA.cps, choices = c(1,2), obs.scale = 1, var.scale = 1, group = data.progress$Adequate.Yearly.Progress.Made., ellipse = TRUE, varname.size=0, var.axes = F) + 
  scale_color_discrete() + 
  theme(legend.direction = 'horizontal', legend.position = 'top') +
  ggtitle("PC Value Separations")
```
###Inspecting PC value weights
```{r}
#In order of PC1 Vlaues
PC.weights <- as.data.frame(PCA.cps$rotation)[,1:2]
PC.weights <- PC.weights[order(abs(PC.weights$PC1), decreasing = TRUE),]
head(PC.weights)

#In order of PC2 Values
PC.weights <- PC.weights[order(abs(PC.weights$PC2), decreasing = TRUE),]
head(PC.weights)
```

*PC1's magnitude emphasizes better safety/environment and test taking.  This definitely did the best job at separating the data between the two.

*PC2 correlates with better teaching, college enrollment, math, and environment


```{r}
# Jason's section
```